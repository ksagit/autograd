{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd\n",
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "from autograd import checkpoint\n",
    "from autograd.extend import primitive\n",
    "\n",
    "import numpy as onp\n",
    "from time import time\n",
    "\n",
    "%load_ext memory_profiler\n",
    "\n",
    "from builtins import range, list as ag_list, tuple as ag_tuple\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "from autograd import grad\n",
    "from autograd.scipy.misc import logsumexp\n",
    "from os.path import dirname, join\n",
    "from autograd.misc.optimizers import adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_rnn_params(input_size, state_size, output_size,\n",
    "                      param_scale=0.01, rs=npr.RandomState(0)):\n",
    "    return {'change': rs.randn(input_size + state_size + 1, state_size) * param_scale,\n",
    "            'predict': rs.randn(state_size + 1, output_size) * param_scale,\n",
    "            'init hiddens': rs.randn(1, state_size) * param_scale,}\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 0.5*(np.tanh(x) + 1.0)   # Output ranges from 0 to 1.\n",
    "\n",
    "def hiddens_to_output_probs(theta, hiddens):\n",
    "    output = concat_and_multiply(theta['predict'], hiddens)\n",
    "    return output - logsumexp(output, axis=1, keepdims=True)\n",
    "\n",
    "def concat_and_multiply(weights, *args):\n",
    "    cat_state = np.hstack(args + (np.ones((args[0].shape[0], 1)),))\n",
    "    return np.dot(cat_state, weights)\n",
    "\n",
    "input_size = 64\n",
    "state_size = 64\n",
    "output_size = 64\n",
    "\n",
    "batch_size = 64\n",
    "seq_len = 512\n",
    "num_checkpoints = 64\n",
    "\n",
    "theta = create_rnn_params(input_size, state_size, output_size)\n",
    "\n",
    "np.random.seed(0)\n",
    "inputs = [np.random.randn(batch_size, input_size) for _ in range(seq_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from autograd.differential_operators import binomial_checkpoint\n",
    "from autograd.builtins import list as ag_list, tuple as ag_tuple\n",
    "\n",
    "def rnn(theta, state, x):  \n",
    "    return np.tanh(concat_and_multiply(theta['change'], x, state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_predict(params, inputs):\n",
    "    num_sequences = inputs[0].shape[0]\n",
    "    hidden_single = npr.RandomState(0).randn(1, state_size) * .01\n",
    "    hidden = np.repeat(hidden_single, num_sequences, axis=0)  \n",
    "    \n",
    "    outputs = [hiddens_to_output_probs(params, hidden)]\n",
    "    \n",
    "    for input in inputs:  # Iterate over time steps.\n",
    "        hidden = rnn(params, hidden, input)\n",
    "        outputs.append(hiddens_to_output_probs(params, hidden))\n",
    "    return outputs\n",
    "\n",
    "loop = binomial_checkpoint(rnn, seq_len, num_checkpoints, hiddens_to_output_probs)\n",
    "\n",
    "def rnn_predict_checkpointed(params, inputs):\n",
    "    num_sequences = inputs[0].shape[0]\n",
    "    \n",
    "    hidden_single = npr.RandomState(0).randn(1, state_size) * .01\n",
    "    hidden = np.repeat(hidden_single, num_sequences, axis=0)\n",
    "    \n",
    "    return loop(params, hidden, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change rnn_predict to rnn_predict_checkpointed and restart the notebook for comparison\n",
    "f = lambda theta: rnn_predict(theta, inputs)\n",
    "g = lambda theta: np.sum(sum(f(theta)))\n",
    "\n",
    "# the first time grad is called introduces memory overhead, so we do it here and ignore it\n",
    "_ = grad(g)(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 218.01 MiB, increment: 60.02 MiB\n",
      "elapsed time:  0.8502910137176514\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "%memit g1 = grad(g)(theta)\n",
    "end = time()\n",
    "\n",
    "print(\"elapsed time: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156.55160449204902\n",
      "-5.4498627832799684e-12\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "for key in g1:\n",
    "    print(np.sum(g1[key]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
